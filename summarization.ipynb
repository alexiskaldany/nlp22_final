{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "import torch\n",
    "from torchmetrics import Perplexity\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"covid_articles_raw_first_250_.csv\").absolute()\n",
    "number_of_samples = 10\n",
    "max_len = 1024\n",
    "df = pd.read_csv(csv_path)[:number_of_samples]\n",
    "# train_dataset = df.sample(frac=0.8, random_state=0)\n",
    "# print(f\"Loaded {len(train_dataset)} rows of training data\")\n",
    "# val_dataset = df.drop(train_dataset.index).sample(frac=0.5, random_state=0)\n",
    "# print(f\"Loaded {len(val_dataset)} rows of validation data\")\n",
    "# test_dataset = df.drop(train_dataset.index).drop(val_dataset.index)\n",
    "# print(f\"Loaded {len(test_dataset)} rows of test data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.dataframe = dataframe\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.dataframe['input_ids'].to_list()[idx],\n",
    "            'attention_mask': self.dataframe['attention_mask'].to_list()[idx],\n",
    "            'labels': self.dataframe['input_ids'].to_list()[idx]\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe['input_ids'])  # len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/alexiskaldany/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/alexiskaldany/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/vocab.json\n",
      "loading file merges.txt from cache at /Users/alexiskaldany/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/alexiskaldany/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/alexiskaldany/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/alexiskaldany/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/alexiskaldany/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/c5121e42f57eca153aea31729f71cbedcd77a656/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           input_ids  \\\n",
      "0  [0, 30086, 6, 99, 32, 47, 546, 13, 116, 50118,...   \n",
      "1  [0, 4030, 469, 431, 10, 638, 1814, 6, 24083, 9...   \n",
      "2  [0, 30086, 6, 99, 32, 47, 546, 13, 116, 50118,...   \n",
      "3  [0, 30086, 6, 99, 32, 47, 546, 13, 116, 50118,...   \n",
      "4  [0, 30086, 6, 99, 32, 47, 546, 13, 116, 50118,...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                              labels  \n",
      "0  [0, 17629, 88, 5, 3157, 59, 2297, 7637, 11534,...  \n",
      "1  [0, 725, 3463, 139, 4885, 4755, 613, 1997, 2, ...  \n",
      "2  [0, 21318, 10369, 6, 2769, 7102, 11, 18900, 59...  \n",
      "3  [0, 36017, 18, 809, 26484, 9923, 81, 33640, 11...  \n",
      "4  [0, 574, 3209, 38230, 9118, 394, 5340, 27157, ...  \n",
      "{'input_ids': [0, 30086, 6, 99, 32, 47, 546, 13, 116, 50118, 2765, 50118, 29642, 50118, 35779, 15, 296, 18000, 5, 5308, 5238, 9, 320, 9767, 3504, 6778, 5977, 366, 282, 54, 5303, 7, 8398, 7, 1877, 10, 2898, 1500, 6, 150, 2769, 5624, 484, 82, 25, 233, 9, 10, 18900, 4513, 88, 5, 573, 28549, 4, 50118, 28916, 366, 282, 6, 54, 2713, 1533, 1103, 9, 613, 6046, 14, 37, 9118, 6, 351, 4852, 11, 587, 53, 19, 8414, 1274, 480, 217, 10, 2020, 15, 4886, 1504, 4, 50118, 1708, 5, 25425, 8988, 1031, 6, 54, 34, 1515, 6, 6606, 8, 17146, 632, 2192, 6, 2312, 7, 9215, 66, 9, 1429, 15, 395, 1135, 519, 4507, 81, 39, 130, 22631, 7, 39, 3969, 4, 50118, 250, 461, 11, 5308, 56, 1220, 5977, 366, 282, 7, 489, 10, 200, 1515, 12373, 25, 37, 956, 65, 7, 1504, 1025, 1429, 6, 10, 1300, 593, 7, 5, 948, 174, 5040, 296, 4, 50118, 49519, 91, 56, 7, 489, 42, 12373, 12801, 7, 3364, 39, 765, 12, 20952, 2194, 6, 5, 1300, 26, 6, 1271, 35, 45518, 345, 21, 5537, 31, 5, 461, 4, 12801, 50118, 28916, 366, 282, 21, 1220, 42, 1515, 12373, 98, 251, 25, 24, 21, 1682, 11, 10, 5930, 403, 19, 5, 3260, 547, 30, 39, 3969, 6, 5, 1300, 26, 4, 50118, 970, 16, 117, 2841, 33836, 414, 2018, 5977, 366, 282, 18, 5824, 31, 1429, 53, 37, 2867, 8398, 15, 10, 1515, 12373, 6, 285, 10901, 18886, 530, 26, 4, 50118, 18551, 2126, 2650, 12021, 32937, 2624, 5, 6089, 4215, 9, 5977, 366, 282, 18, 21832, 5111, 4, 50118, 3762, 2026, 11, 5, 17146, 433, 21, 14, 5, 18248, 21, 31979, 31, 39, 5308, 5238, 11, 10, 4388, 10320, 403, 480, 10, 527, 10, 1300, 11, 39, 3838, 22815, 2296, 4, 50118, 894, 16, 802, 7, 33, 551, 10, 940, 4900, 31, 229, 1253, 1439, 4414, 11, 4669, 1429, 15, 719, 1132, 6, 3393, 13, 12275, 4, 85, 16, 2047, 5977, 366, 282, 3475, 31, 89, 7, 27356, 4, 50118, 33133, 18, 6291, 3158, 34, 1357, 41, 803, 88, 5977, 366, 282, 18, 5890, 2937, 227, 940, 10622, 23, 41, 12275, 3062, 15, 302, 4, 50118, 35779, 5249, 707, 82, 6, 217, 237, 10695, 6, 25, 233, 9, 5, 4513, 6, 340, 1218, 211, 6826, 431, 296, 4, 50118, 12, 4787, 17444, 18000, 111, 50118, 44693, 6, 6637, 6, 18000, 39, 320, 5308, 5238, 296, 25, 233, 9, 41, 2557, 803, 88, 39, 5111, 4, 50118, 16215, 41605, 4338, 969, 484, 1024, 11, 2933, 10401, 4201, 5, 1038, 4, 50118, 29151, 32, 421, 7, 28399, 573, 2280, 4338, 31, 39, 5238, 8, 97, 2127, 51, 1985, 5977, 366, 282, 11252, 7, 137, 37, 5303, 6, 18886, 530, 26, 4, 50118, 9497, 1985, 45518, 484, 12801, 82, 7513, 123, 7, 244, 123, 5111, 45518, 11, 41, 12286, 4737, 6, 12801, 24, 355, 4, 50118, 133, 2898, 168, 34, 648, 7, 696, 143, 781, 445, 15, 5, 403, 4, 50118, 1779, 39, 2994, 3969, 58, 7594, 13, 4852, 6, 3659, 1695, 37, 21, 10, 2524, 810, 19, 2247, 7070, 6, 53, 5977, 366, 282, 1003, 26, 37, 770, 7, 28, 1381, 7, 3364, 39, 18686, 4, 50118, 3762, 9, 39, 3969, 67, 26, 23, 5, 86, 14, 37, 21, 215, 10, 3395, 652, 37, 56, 117, 778, 9, 18991, 409, 35777, 21722, 4, 50118, 29182, 749, 1157, 82, 560, 33, 80, 22631, 9, 5, 276, 26241, 480, 13, 1246, 114, 51, 32, 7690, 18268, 5861, 12075, 13570, 6, 50, 825, 3949, 11, 3050, 19, 349, 97, 4, 50118, 133, 2898, 168, 16, 533, 7, 1394, 8398, 7, 22829, 1459, 5977, 366, 282, 149, 5813, 6237, 6, 53, 3255, 9, 39, 865, 2137, 2082, 11875, 25, 44625, 6439, 34, 117, 16798, 10170, 19, 5308, 4, 50118, 133, 1515, 168, 26, 15, 296, 24, 74, 45, 22829, 1459, 5977, 366, 282, 114, 37, 2035, 11, 5, 247, 142, 24, 473, 45, 22829, 1459, 63, 12437, 4, 50118, 35779, 15, 296, 18000, 5, 5308, 5238, 9, 320, 9767, 3504, 6778, 5977, 366, 282, 54, 5303, 7, 8398, 7, 1877, 10, 2898, 1500, 6, 150, 2769, 5624, 484, 82, 25, 233, 9, 10, 18900, 4513, 88, 5, 573, 28549, 4, 50118, 28916, 366, 282, 6, 54, 2713, 1533, 1103, 9, 613, 6046, 14, 37, 9118, 6, 351, 4852, 11, 587, 53, 19, 8414, 1274, 93, 217, 10, 2020, 15, 4886, 1504, 4, 50118, 1708, 5, 25425, 8988, 1031, 6, 54, 34, 1515, 6, 6606, 8, 17146, 632, 2192, 6, 2312, 7, 9215, 66, 9, 1429, 15, 395, 1135, 519, 4507, 81, 39, 130, 22631, 7, 39, 3969, 4, 50118, 250, 461, 11, 5308, 56, 1220, 5977, 366, 282, 7, 489, 10, 200, 1515, 12373, 25, 37, 956, 65, 7, 1504, 1025, 1429, 6, 10, 1300, 593, 7, 5, 948, 174, 5040, 296, 4, 50118, 17, 48, 91, 56, 7, 489, 42, 12373, 44, 46, 7, 3364, 39, 765, 12, 20952, 2194, 6, 5, 1300, 26, 6, 1271, 35, 44, 48, 345, 21, 5537, 31, 5, 461, 4, 44, 46, 50140, 50118, 28916, 366, 282, 21, 1220, 42, 1515, 12373, 98, 251, 25, 24, 21, 1682, 11, 10, 5930, 403, 19, 5, 3260, 547, 30, 39, 3969, 6, 5, 1300, 26, 4, 50118, 970, 16, 117, 2841, 33836, 414, 2018, 5977, 366, 282, 17, 27, 579, 5824, 31, 1429, 53, 37, 2867, 8398, 15, 10, 1515, 12373, 6, 285, 10901, 18886, 530, 26, 4, 50118, 18551, 2126, 2650, 12021, 32937, 2624, 5, 6089, 4215, 9, 5977, 366, 282, 17, 27, 579, 21832, 5111, 4, 50118, 3762, 2026, 11, 5, 17146, 433, 21, 14, 5, 18248, 21, 31979, 31, 39, 5308, 5238, 11, 10, 4388, 10320, 403, 93, 10, 527, 10, 1300, 11, 39, 3838, 22815, 2296, 4, 50118, 894, 16, 802, 7, 33, 551, 10, 940, 4900, 31, 229, 1253, 1439, 4414, 11, 4669, 1429, 15, 719, 1132, 6, 3393, 13, 12275, 4, 85, 16, 2047, 5977, 366, 282, 3475, 31, 89, 7, 27356, 4, 50118, 33133, 17, 27, 579, 6291, 3158, 34, 1357, 41, 803, 88, 5977, 366, 282, 17, 27, 579, 5890, 2937, 227, 940, 10622, 23, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 30086, 6, 99, 32, 47, 546, 13, 116, 50118, 2765, 50118, 29642, 50118, 35779, 15, 296, 18000, 5, 5308, 5238, 9, 320, 9767, 3504, 6778, 5977, 366, 282, 54, 5303, 7, 8398, 7, 1877, 10, 2898, 1500, 6, 150, 2769, 5624, 484, 82, 25, 233, 9, 10, 18900, 4513, 88, 5, 573, 28549, 4, 50118, 28916, 366, 282, 6, 54, 2713, 1533, 1103, 9, 613, 6046, 14, 37, 9118, 6, 351, 4852, 11, 587, 53, 19, 8414, 1274, 480, 217, 10, 2020, 15, 4886, 1504, 4, 50118, 1708, 5, 25425, 8988, 1031, 6, 54, 34, 1515, 6, 6606, 8, 17146, 632, 2192, 6, 2312, 7, 9215, 66, 9, 1429, 15, 395, 1135, 519, 4507, 81, 39, 130, 22631, 7, 39, 3969, 4, 50118, 250, 461, 11, 5308, 56, 1220, 5977, 366, 282, 7, 489, 10, 200, 1515, 12373, 25, 37, 956, 65, 7, 1504, 1025, 1429, 6, 10, 1300, 593, 7, 5, 948, 174, 5040, 296, 4, 50118, 49519, 91, 56, 7, 489, 42, 12373, 12801, 7, 3364, 39, 765, 12, 20952, 2194, 6, 5, 1300, 26, 6, 1271, 35, 45518, 345, 21, 5537, 31, 5, 461, 4, 12801, 50118, 28916, 366, 282, 21, 1220, 42, 1515, 12373, 98, 251, 25, 24, 21, 1682, 11, 10, 5930, 403, 19, 5, 3260, 547, 30, 39, 3969, 6, 5, 1300, 26, 4, 50118, 970, 16, 117, 2841, 33836, 414, 2018, 5977, 366, 282, 18, 5824, 31, 1429, 53, 37, 2867, 8398, 15, 10, 1515, 12373, 6, 285, 10901, 18886, 530, 26, 4, 50118, 18551, 2126, 2650, 12021, 32937, 2624, 5, 6089, 4215, 9, 5977, 366, 282, 18, 21832, 5111, 4, 50118, 3762, 2026, 11, 5, 17146, 433, 21, 14, 5, 18248, 21, 31979, 31, 39, 5308, 5238, 11, 10, 4388, 10320, 403, 480, 10, 527, 10, 1300, 11, 39, 3838, 22815, 2296, 4, 50118, 894, 16, 802, 7, 33, 551, 10, 940, 4900, 31, 229, 1253, 1439, 4414, 11, 4669, 1429, 15, 719, 1132, 6, 3393, 13, 12275, 4, 85, 16, 2047, 5977, 366, 282, 3475, 31, 89, 7, 27356, 4, 50118, 33133, 18, 6291, 3158, 34, 1357, 41, 803, 88, 5977, 366, 282, 18, 5890, 2937, 227, 940, 10622, 23, 41, 12275, 3062, 15, 302, 4, 50118, 35779, 5249, 707, 82, 6, 217, 237, 10695, 6, 25, 233, 9, 5, 4513, 6, 340, 1218, 211, 6826, 431, 296, 4, 50118, 12, 4787, 17444, 18000, 111, 50118, 44693, 6, 6637, 6, 18000, 39, 320, 5308, 5238, 296, 25, 233, 9, 41, 2557, 803, 88, 39, 5111, 4, 50118, 16215, 41605, 4338, 969, 484, 1024, 11, 2933, 10401, 4201, 5, 1038, 4, 50118, 29151, 32, 421, 7, 28399, 573, 2280, 4338, 31, 39, 5238, 8, 97, 2127, 51, 1985, 5977, 366, 282, 11252, 7, 137, 37, 5303, 6, 18886, 530, 26, 4, 50118, 9497, 1985, 45518, 484, 12801, 82, 7513, 123, 7, 244, 123, 5111, 45518, 11, 41, 12286, 4737, 6, 12801, 24, 355, 4, 50118, 133, 2898, 168, 34, 648, 7, 696, 143, 781, 445, 15, 5, 403, 4, 50118, 1779, 39, 2994, 3969, 58, 7594, 13, 4852, 6, 3659, 1695, 37, 21, 10, 2524, 810, 19, 2247, 7070, 6, 53, 5977, 366, 282, 1003, 26, 37, 770, 7, 28, 1381, 7, 3364, 39, 18686, 4, 50118, 3762, 9, 39, 3969, 67, 26, 23, 5, 86, 14, 37, 21, 215, 10, 3395, 652, 37, 56, 117, 778, 9, 18991, 409, 35777, 21722, 4, 50118, 29182, 749, 1157, 82, 560, 33, 80, 22631, 9, 5, 276, 26241, 480, 13, 1246, 114, 51, 32, 7690, 18268, 5861, 12075, 13570, 6, 50, 825, 3949, 11, 3050, 19, 349, 97, 4, 50118, 133, 2898, 168, 16, 533, 7, 1394, 8398, 7, 22829, 1459, 5977, 366, 282, 149, 5813, 6237, 6, 53, 3255, 9, 39, 865, 2137, 2082, 11875, 25, 44625, 6439, 34, 117, 16798, 10170, 19, 5308, 4, 50118, 133, 1515, 168, 26, 15, 296, 24, 74, 45, 22829, 1459, 5977, 366, 282, 114, 37, 2035, 11, 5, 247, 142, 24, 473, 45, 22829, 1459, 63, 12437, 4, 50118, 35779, 15, 296, 18000, 5, 5308, 5238, 9, 320, 9767, 3504, 6778, 5977, 366, 282, 54, 5303, 7, 8398, 7, 1877, 10, 2898, 1500, 6, 150, 2769, 5624, 484, 82, 25, 233, 9, 10, 18900, 4513, 88, 5, 573, 28549, 4, 50118, 28916, 366, 282, 6, 54, 2713, 1533, 1103, 9, 613, 6046, 14, 37, 9118, 6, 351, 4852, 11, 587, 53, 19, 8414, 1274, 93, 217, 10, 2020, 15, 4886, 1504, 4, 50118, 1708, 5, 25425, 8988, 1031, 6, 54, 34, 1515, 6, 6606, 8, 17146, 632, 2192, 6, 2312, 7, 9215, 66, 9, 1429, 15, 395, 1135, 519, 4507, 81, 39, 130, 22631, 7, 39, 3969, 4, 50118, 250, 461, 11, 5308, 56, 1220, 5977, 366, 282, 7, 489, 10, 200, 1515, 12373, 25, 37, 956, 65, 7, 1504, 1025, 1429, 6, 10, 1300, 593, 7, 5, 948, 174, 5040, 296, 4, 50118, 17, 48, 91, 56, 7, 489, 42, 12373, 44, 46, 7, 3364, 39, 765, 12, 20952, 2194, 6, 5, 1300, 26, 6, 1271, 35, 44, 48, 345, 21, 5537, 31, 5, 461, 4, 44, 46, 50140, 50118, 28916, 366, 282, 21, 1220, 42, 1515, 12373, 98, 251, 25, 24, 21, 1682, 11, 10, 5930, 403, 19, 5, 3260, 547, 30, 39, 3969, 6, 5, 1300, 26, 4, 50118, 970, 16, 117, 2841, 33836, 414, 2018, 5977, 366, 282, 17, 27, 579, 5824, 31, 1429, 53, 37, 2867, 8398, 15, 10, 1515, 12373, 6, 285, 10901, 18886, 530, 26, 4, 50118, 18551, 2126, 2650, 12021, 32937, 2624, 5, 6089, 4215, 9, 5977, 366, 282, 17, 27, 579, 21832, 5111, 4, 50118, 3762, 2026, 11, 5, 17146, 433, 21, 14, 5, 18248, 21, 31979, 31, 39, 5308, 5238, 11, 10, 4388, 10320, 403, 93, 10, 527, 10, 1300, 11, 39, 3838, 22815, 2296, 4, 50118, 894, 16, 802, 7, 33, 551, 10, 940, 4900, 31, 229, 1253, 1439, 4414, 11, 4669, 1429, 15, 719, 1132, 6, 3393, 13, 12275, 4, 85, 16, 2047, 5977, 366, 282, 3475, 31, 89, 7, 27356, 4, 50118, 33133, 17, 27, 579, 6291, 3158, 34, 1357, 41, 803, 88, 5977, 366, 282, 17, 27, 579, 5890, 2937, 227, 940, 10622, 23, 2]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\", max_len = max_len, truncation = True, padding = True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\", max_position_embeddings = max_len,ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "encodings = tokenizer(df[\"content\"].tolist(), max_length=max_len, truncation=True, padding=True)\n",
    "decodings = tokenizer(df[\"title\"].tolist(), max_length=max_len, truncation=True, padding=True)\n",
    "full_df = pd.DataFrame({'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask'], 'labels': decodings['input_ids']})\n",
    "print(full_df.head())\n",
    "train_df = full_df.sample(frac=0.8, random_state=0)\n",
    "train_dataset = BartDataset(train_df)\n",
    "# val = summarizationDataLoader(val_dataset,tokenizer=tokenizer,max_len=max_len)\n",
    "# test = summarizationDataLoader(test_dataset,tokenizer=tokenizer,max_len=max_len)\n",
    "\n",
    "print(train_dataset[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Users/alexiskaldany/school/nlp22_final/.venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c81228412a41fbb76f15f8a2898b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m seq2seqtraining_args \u001b[39m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./summarization\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m seq2seq_trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      7\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      8\u001b[0m     args\u001b[39m=\u001b[39mseq2seqtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m seq2seq_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/transformers/trainer.py:1500\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1497\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1498\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1499\u001b[0m )\n\u001b[0;32m-> 1500\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1501\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1502\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1503\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1504\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1505\u001b[0m )\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/transformers/trainer.py:1716\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1713\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1715\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1716\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1720\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/transformers/trainer_utils.py:696\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[\u001b[39mdict\u001b[39m]):\n\u001b[1;32m    695\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_columns(feature) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features]\n\u001b[0;32m--> 696\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_collator(features)\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py:247\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 247\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    248\u001b[0m         features,\n\u001b[1;32m    249\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    250\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    251\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    252\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    255\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/school/nlp22_final/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2902\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2898\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2899\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m   2900\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2901\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2902\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39;49mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m     )\n\u001b[1;32m   2905\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   2907\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "seq2seqtraining_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summarization\",\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "seq2seq_trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=seq2seqtraining_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "seq2seq_trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "8910b3cece3f3962bcb24bb602e14b8489f27e623b78b93650f4fdc25ed2fb4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
